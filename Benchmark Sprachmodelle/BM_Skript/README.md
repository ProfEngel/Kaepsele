# vLLM Benchmark


Test auf einer L40S: czhkva2g2hii5x - benchmark_results_L40s
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://czhkva2g2hii5x-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://czhkva2g2hii5x-8000.proxy.runpod.net/v1" --api_key "your-api-key"


Test auf einer RTX 6000 Ada: lgcfgr985i85ec - benchmark_results_RTX6000Ada
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://lgcfgr985i85ec-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://lgcfgr985i85ec-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer H100 PCIe: 3cq33hkj1os46e - benchmark_results_H100PCIe
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://3cq33hkj1os46e-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://3cq33hkj1os46e-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer H200 SXM: xnymq70wpgt8ff - benchmark_results_H200SXM
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://xnymq70wpgt8ff-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://xnymq70wpgt8ff-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer L40: r0lttmm5ucun7f - benchmark_results_L40
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://r0lttmm5ucun7f-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://r0lttmm5ucun7f-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer RTX A4500: dxkqg0t0rso0we - benchmark_results_RTX_A4500
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://dxkqg0t0rso0we-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://dxkqg0t0rso0we-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer RTX 4090: 6xbxzh4cmj4tum - benchmark_results_RTX_4090
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://6xbxzh4cmj4tum-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://6xbxzh4cmj4tum-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer L4: 9bk2sqxqv1mwal - benchmark_results_L4
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://9bk2sqxqv1mwal-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://9bk2sqxqv1mwal-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer RTX4000Ada: pxkd2kfwtbxm8x - benchmark_results_RTX4000Ada
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://pxkd2kfwtbxm8x-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://pxkd2kfwtbxm8x-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer A100PCIe: dm76m944dhoc6i - benchmark_results_A100PCIe
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://dm76m944dhoc6i-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://dm76m944dhoc6i-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer A40: 1pb4c7v52jp408 - benchmark_results_A40
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://1pb4c7v52jp408-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://1pb4c7v52jp408-8000.proxy.runpod.net/v1" --api_key "your-api-key"

A
Test auf einer RTX A5000: vl5ukugl8dmz7q - benchmark_results_RTX_A5000
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://vl5ukugl8dmz7q-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://vl5ukugl8dmz7q-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer RTX A6000: nmyzthf2i9u5v8 - benchmark_results_RTX_A6000
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "https://nmyzthf2i9u5v8-8000.proxy.runpod.net/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "https://nmyzthf2i9u5v8-8000.proxy.runpod.net/v1" --api_key "your-api-key"

Test auf einer A10 (IONOS): vl5ukugl8dmz7q - benchmark_results_A10
Einzeltest:
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "http://85.215.141.142:8000/v1" --api_key "none"
Voll RUN: mit 1, 10, 50,100
python run_benchmarks.py --vllm_url "http://85.215.141.142:8000/v1" --api_key "your-api-key"




















This repository contains scripts for benchmarking the performance of large language models (LLMs) served using vLLM. It's designed to test the scalability and performance of LLM deployments under various concurrency levels.

## Features

- Benchmark LLMs with different concurrency levels
- Measure key performance metrics:
  - Requests per second
  - Latency
  - Tokens per second
  - Time to first token
- Easy to run with customizable parameters
- Generates JSON output for further analysis or visualization

## Requirements

- Python 3.7+
- `openai` Python package
- `numpy` Python package

## Installation

1. Clone this repository:
   ```
   git clone https://github.com/yourusername/vllm-benchmark.git
   cd vllm-benchmark
   ```

2. Install the required packages:
   ```
   pip install openai numpy
   ```

## Usage

### Single Benchmark Run

To run a single benchmark:

```
python vllm_benchmark.py --num_requests 100 --concurrency 10 --output_tokens 100 --vllm_url "http://localhost:8000/v1" --api_key "your-api-key"
```

Parameters:
- `num_requests`: Total number of requests to make
- `concurrency`: Number of concurrent requests
- `output_tokens`: Number of tokens to generate per request
- `vllm_url`: URL of the vLLM server
- `api_key`: API key for the vLLM server
- `request_timeout`: (Optional) Timeout for each request in seconds (default: 30)

### Multiple Benchmark Runs

To run multiple benchmarks with different concurrency levels:

```
python run_benchmarks.py --vllm_url "http://localhost:8000/v1" --api_key "your-api-key"
```

This script will run benchmarks with concurrency levels of 1, 10, 50, and 100, and save the results to `benchmark_results.json`.

## Output

The benchmark results are saved in JSON format, containing detailed metrics for each run, including:

- Total requests and successful requests
- Requests per second
- Total output tokens
- Latency (average, p50, p95, p99)
- Tokens per second (average, p50, p95, p99)
- Time to first token (average, p50, p95, p99)

## Results

Please see the results directory for benchmarks on [Backprop](https://backprop.co) instances.

## Contributing

Contributions to improve the benchmarking scripts or add new features are welcome! Please feel free to submit pull requests or open issues for any bugs or feature requests.

## License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.
