# AI-Tutor f√ºr die Hochschullehre - GPU-Server Setup f√ºr On-Premise Sprachmodelle

## üìö √úber das Projekt

Dieses Setup ist Teil des **Tandemforschungsprojekts** von **Tobias Leiblein** und **Prof. Dr. Mathias Engel** (Hochschule f√ºr Wirtschaft und Umwelt N√ºrtingen-Geislingen), gef√∂rdert durch das **Fellowship f√ºr Lehrinnovationen und Unterst√ºtzungsangebote in der digitalen Hochschullehre Baden-W√ºrttemberg**.

### Projektzielsetzung

Das Forschungsprojekt entwickelt und erprobt einen **KI-Tutor f√ºr die Hochschullehre**, der mithilfe von **OpenTuneWeaver** - einer eigens entwickelten Trainingspipeline - optimierte und f√ºr die Lehre angepasste Sprachmodelle bereitstellt. 

**Kernziele:**
- **Qualitative Bewertung:** Analyse des Lernoutputs und der Akzeptanz durch Lehrende
- **Didaktische Integration:** Entwicklung hochschuldidaktischer Angebote zur Technologie-Integration
- **Skalierbarkeit:** Erhebung von Ressourcennutzungsdaten als Grundlage f√ºr eine landesweite Bereitstellung
- **Pr√§-Post-Kompetenzmatrix:** Bestimmung der optimalen Lehrziele und Anwendungsszenarien

### OpenTuneWeaver - Die Trainingspipeline

[**OpenTuneWeaver**](https://github.com/ProfEngel/OpenTuneWeaver) ist ein Framework zur:
- **Datensatzkonvertierung** von PDFs zu strukturierten Q&A-Formaten
- **Finetuning von Open-Source-LLMs** f√ºr spezifische Lehrdom√§nen
- **Automatisierter Modellgenerierung** mit anwenderfreundlicher UI
- **GGUF-Export** f√ºr direkte lokale Nutzung in OpenWebUI

**Technische Features:**
- Integration von QLORA und LORA Fine-tuning
- Realtime-Metriken und Loss-Kurven-Monitoring
- Automatischer Hugging Face Model-Upload
- Support f√ºr verschiedene Trainingsdatenformate (Q&A, Chat, Function Calling)

---

**Gef√∂rdert von:**

<div style="display: flex; align-items: center; gap: 30px; margin: 20px 0;">
  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Stifterverband_f%C3%BCr_die_Deutsche_Wissenschaft_Logo_02.2022.svg/300px-Stifterverband_f%C3%BCr_die_Deutsche_Wissenschaft_Logo_02.2022.svg.png" alt="Stifterverband f√ºr die Deutsche Wissenschaft" style="height: 60px;">
  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Greater_coat_of_arms_of_Baden-W%C3%BCrttemberg.svg/80px-Greater_coat_of_arms_of_Baden-W%C3%BCrttemberg.svg.png" alt="Ministerium f√ºr Wissenschaft, Forschung und Kunst Baden-W√ºrttemberg" style="height: 60px;">
</div>

**Stifterverband f√ºr die Deutsche Wissenschaft** und **Ministerium f√ºr Wissenschaft, Forschung und Kunst Baden-W√ºrttemberg**

---

# Einrichtung eines GPU-Servers zur Bereitstellung eines On-Premise-Sprachmodells mittels OpenWebUI

## ‚ö†Ô∏è Sicherheitshinweise

**KRITISCH: Diese Anleitung wurde f√ºr Produktionsumgebungen √ºberarbeitet**

- **Niemals** Credentials, API-Keys oder Passw√∂rter in Klartext verwenden
- Alle Platzhalter `YOUR_*` und `<REPLACE_*>` durch echte Werte ersetzen
- Sichere Passw√∂rter und API-Keys generieren
- Firewall-Regeln restriktiv konfigurieren
- Services nicht als root laufen lassen (wo m√∂glich)
- Regelm√§√üige Updates und Sicherheitspatches einspielen

## 0. Firewall-Konfiguration

### Ben√∂tigte Ports

| Port | Service | Extern zug√§nglich | Beschreibung |
|------|---------|------------------|--------------|
| 80 | OpenWebUI | ‚úÖ | Web-Interface f√ºr Chat |
| 8000-8010 | vLLM/Models | ‚ö†Ô∏è | API-Endpunkte (nur bei Bedarf) |
| 11434 | Ollama | ‚ö†Ô∏è | Embedding-/Vision-Modelle |
| 3000 | Perplexica Frontend | ‚úÖ | Alternative Search UI |
| 3001 | Perplexica Backend | ‚ùå | Interne API |
| 4000 | SearXNG | ‚ö†Ô∏è | Metasuchmaschine |
| 7870 | Stable Diffusion | ‚ö†Ô∏è | Bildgenerierung |
| 8888 | Jupyter | ‚ùå | Code-Interpreter |
| 9099 | OpenWebUI Pipelines | ‚ùå | Interne Pipelines |

### Firewall einrichten

```bash
# UFW (Uncomplicated Firewall) zur√ºcksetzen - entfernt alle vorherigen Regeln
sudo ufw --force reset

# Standardregeln: Eingehende Verbindungen blockieren, ausgehende erlauben
sudo ufw default deny incoming
sudo ufw default allow outgoing

# SSH-Zugang sicherstellen (anpassen an Ihren SSH-Port)
# WICHTIG: Ohne diese Regel verlieren Sie SSH-Zugang!
sudo ufw allow 22/tcp

# Nur notwendige Ports f√ºr den AI-Stack √∂ffnen
sudo ufw allow 80/tcp    # OpenWebUI - f√ºr Web-Interface des Chatbots
# sudo ufw allow 3000/tcp  # Perplexica (nur bei Bedarf) - f√ºr erweiterte Suchfunktionen

# UFW aktivieren - ab jetzt sind die Regeln aktiv
sudo ufw --force enable

# Status anzeigen - zeigt alle aktiven Firewall-Regeln
sudo ufw status verbose
```
*Diese Konfiguration wird **zu Beginn einmalig** ausgef√ºhrt und sichert den Server ab.*

## 1. Grundlegende Systemkonfiguration

### System-Updates und Tools

```bash
# System aktualisieren - holt neueste Sicherheitsupdates und Paketinformationen
sudo apt update && sudo apt upgrade -y

# Grundlegende Entwicklungstools installieren
sudo apt install -y \
    build-essential \    # Compiler und Build-Tools f√ºr Software-Kompilierung
    wget \              # Download-Tool f√ºr Dateien aus dem Internet
    curl \              # Vielseitiges Tool f√ºr HTTP-Requests und Downloads
    git \               # Versionskontrolle f√ºr Code-Repositories
    htop \              # Erweiterte System√ºberwachung (besseres top)
    ufw \               # Firewall-Management
    fail2ban \          # Schutz vor Brute-Force-Angriffen
    unattended-upgrades # Automatische Sicherheitsupdates

# Automatische Sicherheitsupdates aktivieren - System h√§lt sich selbst aktuell
sudo dpkg-reconfigure -plow unattended-upgrades
```
*Diese Befehle werden **einmalig** nach der Server-Installation ausgef√ºhrt.*

## 2. Miniconda-Installation

```bash
# Miniconda-Installer herunterladen (neueste Version f√ºr Linux x86_64)
# Miniconda = minimale Python-Distribution mit Paketmanager conda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# Installation im Batch-Modus (keine Benutzerinteraktion erforderlich)
# -b = Batch-Modus, -p = Installationspfad festlegen
bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3

# Conda f√ºr aktuelle Shell-Sitzung initialisieren
# F√ºgt conda-Befehle zur PATH-Variable hinzu
~/miniconda3/bin/conda init

# Shell neu starten um conda-Konfiguration zu laden
exec bash

# Conda-Version pr√ºfen - verifiziert erfolgreiche Installation
conda --version

# Conda selbst aktualisieren auf neueste Version
conda update -n base -c defaults conda -y
```
*Diese Schritte werden **einmalig** ausgef√ºhrt und richten die Python-Umgebung ein.*

## 3. NVIDIA GPU-Setup

### GPU-Treiber installieren (Aktuell: 570.x/575.x)

```bash
# GPU-Hardware-Erkennung - zeigt alle NVIDIA-Karten im System
lspci | grep -i nvidia

# Option 1: Stabile Production Branch installieren (empfohlen f√ºr Server)
# 570er Serie = langzeitunterst√ºtzt, getestet, stabil
sudo apt install -y nvidia-driver-570

# System neustarten - Treiber wird erst nach Reboot aktiv
sudo reboot

# Option 2: New Feature Branch (f√ºr neueste Features, nur wenn n√∂tig)
# 575er Serie = neueste Features, aber weniger getestet
# sudo apt install -y nvidia-driver-575
# sudo reboot

# Nach Neustart: GPU-Status und Treiberinfo anzeigen
nvidia-smi  # Zeigt GPU-Auslastung, Temperatur, Speicher

# Genaue Treiber-Version anzeigen
nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits
```
*Wird **einmalig** nach der Basis-Installation ausgef√ºhrt. **Neustart erforderlich!***

### CUDA-Toolkit installieren (Aktuell: 12.9)

#### F√ºr Ubuntu 22.04:
```bash
# NVIDIA CUDA Repository-Schl√ºssel herunterladen und installieren
# Erm√∂glicht sicheren Download von CUDA-Paketen
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb

# Paketlisten aktualisieren - l√§dt CUDA-Repository-Informationen
sudo apt-get update

# CUDA Toolkit 12.9 installieren - vollst√§ndige Entwicklungsumgebung
# Ben√∂tigt ~6GB Speicherplatz
sudo apt-get -y install cuda-toolkit-12-9
```

#### F√ºr Ubuntu 24.04:
```bash
# Gleicher Prozess f√ºr Ubuntu 24.04 (neuere Repository-URL)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-9
```

### CUDA-Umgebung konfigurieren

```bash
# CUDA-Binaries zur PATH-Variable hinzuf√ºgen
# Erm√∂glicht Verwendung von nvcc (CUDA-Compiler) und anderen Tools
cat >> ~/.bashrc << 'EOF'
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
EOF

# Umgebungsvariablen sofort laden (ohne Shell-Neustart)
source ~/.bashrc

# CUDA-Compiler-Version pr√ºfen - verifiziert erfolgreiche Installation
nvcc --version
```
*Diese Konfiguration ist **einmalig** nach CUDA-Installation erforderlich.*

## 4. Docker und Container-Runtime

### Docker installieren

```bash
# Docker Engine und Docker Compose aus Ubuntu-Repository installieren
# Einfacher als Docker's eigenes Repository, ausreichend f√ºr unsere Zwecke
sudo apt install -y docker.io docker-compose

# Docker-Service starten und f√ºr Autostart konfigurieren
sudo systemctl start docker    # Startet Docker sofort
sudo systemctl enable docker   # Startet Docker automatisch beim Bootvorgang

# NVIDIA Container Toolkit installieren - erm√∂glicht GPU-Zugriff in Containern
# GPG-Schl√ºssel f√ºr sicheren Download hinzuf√ºgen
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
    sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

# Repository zur Paketliste hinzuf√ºgen
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

# Paketlisten aktualisieren und NVIDIA Container Toolkit installieren
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# Docker f√ºr GPU-Nutzung konfigurieren
sudo nvidia-ctk runtime configure --runtime=docker

# Docker-Services neustarten um Konfiguration zu aktivieren
sudo systemctl restart docker

# Docker Compose von GitHub herunterladen (neueste Version)
COMPOSE_VERSION=$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep -o '"tag_name": "[^"]*' | cut -d'"' -f4)
sudo curl -L "https://github.com/docker/compose/releases/download/${COMPOSE_VERSION}/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

# Docker Compose ausf√ºhrbar machen
sudo chmod +x /usr/local/bin/docker-compose

# Installation verifizieren
docker --version        # Zeigt Docker-Version
docker-compose --version  # Zeigt Docker Compose-Version
```
*Diese Installation erfolgt **einmalig** und ist Grundlage f√ºr alle Container-Services.*

### Benutzer f√ºr Services erstellen

```bash
# WICHTIG: Erst NACH Docker-Installation ausf√ºhren!
# Dedizierten Benutzer f√ºr AI-Services erstellen (Security Best Practice)
# -r = System-User (keine Login-Shell), -s = Shell festlegen, -m = Home-Verzeichnis erstellen
sudo useradd -r -s /bin/false -m aiservices

# Benutzer zur Docker-Gruppe hinzuf√ºgen - erlaubt Docker-Container-Verwaltung
# Die docker-Gruppe existiert erst nach Docker-Installation!
sudo usermod -aG docker aiservices

# √úberpr√ºfen ob User korrekt zur Gruppe hinzugef√ºgt wurde
groups aiservices  # Sollte "aiservices : aiservices docker" anzeigen
```
*Wird **einmalig nach Docker-Installation** ausgef√ºhrt. Services laufen nicht als root = sicherer.*

## 5. Credential-Management

### Sichere API-Keys generieren

```bash
# Sicheres Verzeichnis f√ºr alle Credentials erstellen
mkdir -p ~/.ai-credentials
chmod 700 ~/.ai-credentials  # Nur Owner kann lesen/schreiben/ausf√ºhren

echo "Generiere sichere API-Keys..."

# Haupt-API-Key f√ºr vLLM/Modelle generieren (64 Zeichen Hex)
# Wird f√ºr Authentifizierung bei Sprachmodell-APIs verwendet
MAIN_API_KEY="sk-$(openssl rand -hex 32)"
echo "$MAIN_API_KEY" > ~/.ai-credentials/main_api_key

# Jupyter-Token f√ºr Code-Interpreter generieren
# Sichert Zugang zum Jupyter-Notebook-Server
JUPYTER_TOKEN=$(openssl rand -hex 32)
echo "$JUPYTER_TOKEN" > ~/.ai-credentials/jupyter_token

# StableDiffusion-Credentials f√ºr Bildgenerierung
SD_USERNAME="admin"
SD_PASSWORD=$(openssl rand -base64 24)  # Base64 f√ºr komplexere Zeichen
echo "$SD_USERNAME:$SD_PASSWORD" > ~/.ai-credentials/sd_auth

# Perplexica-Secret f√ºr Suchfunktionen
PERPLEXICA_SECRET=$(openssl rand -hex 32)
echo "$PERPLEXICA_SECRET" > ~/.ai-credentials/perplexica_secret

# Alle Credential-Dateien vor unbefugtem Zugriff sch√ºtzen
chmod 600 ~/.ai-credentials/*

echo "API-Keys generiert und gespeichert in ~/.ai-credentials/"
echo "WICHTIG: Diese Keys sicher aufbewahren und niemals in Code/Logs committen!"
```
*Wird **einmalig** ausgef√ºhrt. Diese Keys werden von allen Services verwendet.*

### Umgebungsvariablen-Datei erstellen

```bash
# Docker-Umgebungsdatei mit Pfaden zu Secret-Dateien erstellen
# Verwendet Docker Secrets statt Klartext-Variablen
cat > ~/.ai-credentials/docker.env << EOF
# Generiert am: $(date)
MAIN_API_KEY_FILE=/run/secrets/main_api_key
JUPYTER_TOKEN_FILE=/run/secrets/jupyter_token
SD_AUTH_FILE=/run/secrets/sd_auth
PERPLEXICA_SECRET_FILE=/run/secrets/perplexica_secret

# Netzwerk-Konfiguration - ANPASSEN an Ihre Server-IP!
HOST_IP=YOUR_SERVER_IP  # √ÑNDERN: z.B. 192.168.1.100
INTERNAL_HOST=host.docker.internal

# Service-Port-Konfiguration
OLLAMA_HOST=0.0.0.0:11434
VLLM_HOST=0.0.0.0:8000
WEBUI_PORT=80
EOF

chmod 600 ~/.ai-credentials/docker.env
```

**Server-IP herausfinden:**
```bash
# Interne IP-Adresse des Servers anzeigen
ip addr show | grep -E "inet.*eth0|inet.*wlan0" | awk '{print $2}' | cut -d/ -f1

# Oder spezifischer f√ºr Ethernet-Verbindung:
hostname -I | awk '{print $1}'

# Beispiel-Ausgabe: 192.168.1.100
# Diese IP dann in docker.env eintragen:
# HOST_IP=192.168.1.100

# Externe IP f√ºr Internet-Zugriff (falls ben√∂tigt):
curl -s ifconfig.me  # Zeigt √∂ffentliche IP
```
*Diese IP-Adresse wird in **allen Container-Konfigurationen** verwendet f√ºr Netzwerk-Verbindungen.*

## 6. OpenWebUI mit sicherem Setup

### OpenWebUI mit normalem Docker starten

```bash
# OpenWebUI als einzelner Docker-Container starten (einfacher f√ºr Updates)
# Alle erforderlichen Features in einem Befehl
docker run -d \
  --name open-webui \
  --restart unless-stopped \
  -p 80:8080 \
  -v open-webui:/app/backend/data \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  -e WEBUI_NAME="AI-Tutor f√ºr die Hochschullehre" \
  -e ENABLE_COMMUNITY_SHARING=false \
  -e ENABLE_SIGNUP=false \
  --add-host=host.docker.internal:host-gateway \
  ghcr.io/open-webui/open-webui:main

# Container-Status √ºberpr√ºfen
docker ps | grep open-webui

# Logs anzeigen um sicherzustellen dass alles funktioniert
docker logs open-webui -f
```
*Startet OpenWebUI als **einzelnen Container** - einfacher zu verwalten und zu aktualisieren.*

### Code-Interpreter mit Jupyter (Endlosschleifen-gesch√ºtzt)

```bash
# Jupyter-Container f√ºr Code-Ausf√ºhrung starten
# Verwendet sicheres Token aus unseren Credentials
JUPYTER_TOKEN=$(cat ~/.ai-credentials/jupyter_token)

# Container mit Timeout-Schutz und Resource-Limits starten
docker run -d \
  --name jupyter-interpreter \
  --restart unless-stopped \
  -p 127.0.0.1:8888:8888 \
  -v jupyter-data:/home/jovyan/work \
  -e JUPYTER_ENABLE_LAB=yes \
  -e JUPYTER_TOKEN="$JUPYTER_TOKEN" \
  --cpus="2.0" \
  --memory="4g" \
  --ulimit nofile=1024:1024 \
  jupyter/base-notebook:latest \
  bash -c "
    pip install pandas numpy scipy matplotlib seaborn scikit-learn requests beautifulsoup4 && 
    start-notebook.sh 
    --NotebookApp.ip=0.0.0.0 
    --NotebookApp.port=8888 
    --NotebookApp.token=$JUPYTER_TOKEN
    --NotebookApp.disable_check_xsrf=True
    --NotebookApp.allow_root=True
    --NotebookApp.shutdown_no_activity_timeout=1800
  "

# Container-Status pr√ºfen
docker ps | grep jupyter

# Jupyter-Zugang im Browser: http://YOUR_SERVER_IP:8888
# Token eingeben: [Inhalt aus ~/.ai-credentials/jupyter_token]
```
*Jupyter l√§uft **nur auf localhost** mit **Resource-Limits** gegen Endlosschleifen.*

### Bibliotheken im Jupyter-Notebook installieren

**Nach dem ersten Jupyter-Start im Browser:**

1. **Neues Notebook erstellen** (Python 3)
2. **Erste Zelle:** Bibliotheken installieren
```python
# Kern-Bibliotheken f√ºr Datenanalyse und Wissenschaft
!pip install --upgrade pip

# Datenverarbeitung und -analyse
!pip install pandas          # Datenmanipulation und -analyse
!pip install numpy           # Numerische Berechnungen und Arrays
!pip install scipy          # Wissenschaftliche Berechnungen

# Visualisierung und Plotting
!pip install matplotlib     # Basis-Plotting-Bibliothek
!pip install seaborn        # Statistische Datenvisualisierung
!pip install plotly         # Interaktive Graphiken

# Machine Learning und AI
!pip install scikit-learn   # Machine Learning Algorithmen
!pip install torch          # PyTorch f√ºr Deep Learning
!pip install transformers   # Hugging Face Transformers

# Dokumentenverarbeitung
!pip install pypdf          # PDF-Dateien lesen und verarbeiten
!pip install python-docx    # Word-Dokumente verarbeiten
!pip install openpyxl       # Excel-Dateien lesen/schreiben
!pip install xlsxwriter     # Excel-Dateien erstellen

# Web und APIs
!pip install requests       # HTTP-Requests und API-Aufrufe
!pip install beautifulsoup4 # HTML/XML-Parsing und Web-Scraping
!pip install httpx          # Moderner HTTP-Client

# Utilities und Tools
!pip install qrcode         # QR-Code-Generierung
!pip install pillow         # Bildverarbeitung
!pip install python-pptx    # PowerPoint-Pr√§sentationen
!pip install reportlab      # PDF-Generierung

print("‚úÖ Alle Bibliotheken erfolgreich installiert!")
```

3. **Zweite Zelle:** Installation testen
```python
# Test der wichtigsten Bibliotheken
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests

print("üéâ Jupyter-Umgebung ist bereit f√ºr AI-Tutor-Entwicklung!")

# Beispiel-Plot erstellen
data = np.random.randn(100)
plt.figure(figsize=(8, 4))
plt.hist(data, bins=20, alpha=0.7)
plt.title("Test-Visualisierung")
plt.show()
```

**Endlosschleifen-Schutz aktivieren:**
```python
# In jeder Code-Zelle diese Timeout-Funktion verwenden:
import signal
import time

def timeout_handler(signum, frame):
    raise TimeoutError("Code-Ausf√ºhrung nach 30 Sekunden abgebrochen!")

# Timeout f√ºr Endlosschleifen-Schutz setzen
signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(30)  # 30 Sekunden Timeout

# Hier Ihren Code einf√ºgen...
# for i in range(1000000):  # Beispiel-Code
#     time.sleep(0.001)

signal.alarm(0)  # Timeout zur√ºcksetzen
print("Code erfolgreich ausgef√ºhrt!")
```
*Diese Konfiguration **sch√ºtzt vor Endlosschleifen** und bietet alle n√∂tigen Bibliotheken.*

### Automatische Updates mit Watchtower

```bash
# Manuelles Update von OpenWebUI testen
# Watchtower pr√ºft auf neue Images und aktualisiert automatisch
docker run --rm \
  --volume /var/run/docker.sock:/var/run/docker.sock \
  containrrr/watchtower \
  --run-once open-webui

# Automatische w√∂chentliche Updates einrichten (Freitag 16:00 Uhr)
# Crontab-Eintrag erstellen f√ºr regelm√§√üige Updates
(crontab -l 2>/dev/null; echo "0 16 * * 5 docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui") | crontab -

# Crontab-Eintrag verifizieren
crontab -l | grep watchtower

# Optional: Update-Log erstellen f√ºr Nachverfolgung
(crontab -l 2>/dev/null | grep -v watchtower; echo "0 16 * * 5 docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui >> /var/log/openwebui-updates.log 2>&1") | crontab -

# Log-Datei erstellen
sudo touch /var/log/openwebui-updates.log
sudo chmod 644 /var/log/openwebui-updates.log
```
*Updates laufen **automatisch jeden Freitag um 16:00** - ideal f√ºr Wochenend-Wartung.*

### Update-Status √ºberwachen

```bash
# Aktuellen OpenWebUI-Container und Image-Version anzeigen
docker ps --format "table {{.Names}}\t{{.Image}}\t{{.Status}}" | grep open-webui

# Update-History in Logs pr√ºfen
tail -f /var/log/openwebui-updates.log

# Manuelles Update f√ºr sofortige Aktualisierung
docker run --rm \
  --volume /var/run/docker.sock:/var/run/docker.sock \
  containrrr/watchtower \
  --run-once open-webui \
  --cleanup  # Entfernt alte Images nach Update
```
*Erm√∂glicht **einfache √úberwachung** des Update-Prozesses und manueller Updates bei Bedarf.*

## 7. Modell-Deployment: vLLM vs Ollama

### üöÄ vLLM - Hochperformante Produktionsumgebung

**vLLM ist optimiert f√ºr:**
- **Maximale Durchsatzrate** und niedrige Latenz in Produktionsumgebungen
- **Batch Processing** mit optimaler GPU-Auslastung
- **Advanced Features** wie speculative decoding, kontinuierliches batching
- **Enterprise-Grade APIs** mit OpenAI-Kompatibilit√§t
- **Quantisierte Modelle** (INT8, FP8) f√ºr bessere Effizienz

### üîß Ollama - Entwicklungsfreundliche Umgebung

**Ollama ist ideal f√ºr:**
- **Einfache Installation** und schnelles Prototyping
- **Multi-Modell-Management** (Text, Vision, Embedding)
- **Development Workflows** mit einfacher API
- **Resource Management** mit automatischem Model Loading/Unloading
- **Community-Fokus** mit vielen verf√ºgbaren Modellen

---

## 7.1 vLLM-Deployment (Produktionsumgebung)

## 7.1 vLLM-Deployment (Produktionsumgebung)

### Schritt 1: Conda-Umgebung vorbereiten

```bash
# Dedizierte Conda-Umgebung f√ºr vLLM erstellen
# Isoliert vLLM-Dependencies von anderen Python-Paketen
conda create -n vllm-gemma python=3.10 -y

# Umgebung aktivieren
conda activate vllm-gemma

# Best√§tigung der aktiven Umgebung
conda info --envs | grep "*"
```
*Erstellt **isolierte Python-Umgebung** f√ºr saubere vLLM-Installation.*

### Schritt 2: vLLM und Dependencies installieren

```bash
# vLLM mit Flash Attention installieren (f√ºr bessere Performance)
pip install vllm

# Flash Attention f√ºr Ampere/Ada GPUs (RTX 30/40-Serie)
pip install flash-attn

# Installation verifizieren
python -c "import vllm; print('‚úÖ vLLM erfolgreich installiert')"
```
*Installiert **hochperformante Inference-Engine** mit GPU-Optimierungen.*

### Schritt 3: Systemd-Service erstellen

```bash
# Service-Datei f√ºr automatischen Start erstellen
sudo nano /etc/systemd/system/vllm-gemma.service
```

**Service-Konfiguration einf√ºgen:**
```ini
[Unit]
Description=vLLM Gemma 3 12B INT8 Production Server
Documentation=https://docs.vllm.ai/
After=network.target nvidia-persistenced.service  # Wartet auf Netzwerk und GPU-Treiber
Wants=network.target

[Service]
Type=exec
User=aiservices  # L√§uft nicht als root = sicherer
Group=aiservices
WorkingDirectory=/home/aiservices

# Umgebungsvariablen f√ºr vLLM
Environment="PATH=/root/miniconda3/envs/vllm-gemma/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
Environment="CUDA_VISIBLE_DEVICES=0"  # Verwendet erste GPU
Environment="VLLM_WORKER_MULTIPROC_METHOD=spawn"  # F√ºr Stabilit√§t
Environment="VLLM_LOGGING_LEVEL=INFO"  # Detaillierte Logs

# vLLM-Server starten mit Gemma 3 12B INT8-Modell
ExecStart=/root/miniconda3/envs/vllm-gemma/bin/vllm serve \
  RedHatAI/gemma-3-12b-it-quantized.w8a8 \
  --host 127.0.0.1 \
  --port 8000 \
  --api-key-file /home/aiservices/.ai-credentials/main_api_key \
  --served-model-name gemma-3-12b \
  --max-model-len 4096 \
  --max-num-seqs 16 \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.9 \
  --enable-chunked-prefill \
  --max-num-batched-tokens 8192

# Automatischer Neustart bei Fehlern
Restart=always
RestartSec=10
StartLimitInterval=300
StartLimitBurst=5

# Sicherheitseinstellungen
NoNewPrivileges=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/home/aiservices/.cache
PrivateTmp=true

# Resource-Limits (Gemma 3 12B ben√∂tigt ~16GB GPU VRAM + ~32GB RAM)
MemoryMax=32G
CPUQuota=400%

[Install]
WantedBy=multi-user.target
```

### Schritt 4: Berechtigungen konfigurieren

```bash
# API-Key f√ºr aiservices-User kopieren (Service l√§uft als dieser User)
sudo mkdir -p /home/aiservices/.ai-credentials
sudo cp ~/.ai-credentials/main_api_key /home/aiservices/.ai-credentials/

# Dateiberechtigungen setzen
sudo chown -R aiservices:aiservices /home/aiservices/.ai-credentials
sudo chmod 700 /home/aiservices/.ai-credentials
sudo chmod 600 /home/aiservices/.ai-credentials/main_api_key

# Berechtigungen verifizieren
sudo ls -la /home/aiservices/.ai-credentials/
```
*Konfiguriert **sichere Dateiberechtigungen** f√ºr Service-Account.*

### Schritt 5: Service aktivieren und starten

```bash
# Systemd-Konfiguration neu laden
sudo systemctl daemon-reload

# Service f√ºr automatischen Start aktivieren
sudo systemctl enable vllm-gemma

# Service sofort starten
sudo systemctl start vllm-gemma

# Service-Status pr√ºfen
sudo systemctl status vllm-gemma
```

### Schritt 6: Installation verifizieren

```bash
# Live-Logs anzeigen - zeigt Startvorgang und eventuelle Fehler
sudo journalctl -u vllm-gemma -f

# API-Funktionalit√§t testen
curl -X POST "http://127.0.0.1:8000/v1/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $(cat ~/.ai-credentials/main_api_key)" \
  -d '{
    "model": "gemma-3-12b",
    "prompt": "Erkl√§re mir maschinelles Lernen in einfachen Worten:",
    "max_tokens": 200,
    "temperature": 0.7
  }'

# Erfolgreiche Antwort sollte JSON mit "choices" enthalten
```
*Dieser Service l√§uft **permanent** und startet automatisch nach Server-Neustart.*

### Alternative: vLLM mit Docker (Secure)

```bash
# Docker-basierte vLLM-Konfiguration
cat > ~/docker/vllm-gemma-compose.yml << 'EOF'
version: '3.8'

services:
  vllm-gemma:
    image: vllm/vllm-openai:latest
    container_name: vllm-gemma-prod
    restart: unless-stopped
    ports:
      - "127.0.0.1:8000:8000"  # Nur localhost
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_LOGGING_LEVEL=INFO
    runtime: nvidia
    command: >
      --model RedHatAI/gemma-3-12b-it-quantized.w8a8
      --host 0.0.0.0
      --port 8000
      --api-key-file /run/secrets/main_api_key
      --served-model-name gemma-3-12b
      --max-model-len 4096
      --max-num-seqs 16
      --gpu-memory-utilization 0.9
      --enable-chunked-prefill
      --max-num-batched-tokens 8192
    secrets:
      - main_api_key
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

secrets:
  main_api_key:
    file: ~/.ai-credentials/main_api_key
EOF

# vLLM starten
docker-compose -f ~/docker/vllm-gemma-compose.yml up -d

# Performance-Test
curl -X POST "http://127.0.0.1:8000/v1/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $(cat ~/.ai-credentials/main_api_key)" \
  -d '{
    "model": "gemma-3-12b",
    "prompt": "Erkl√§re mir maschinelles Lernen in einfachen Worten:",
    "max_tokens": 200,
    "temperature": 0.7
  }'
```

## 7.2 Ollama f√ºr Embedding- und Vision-Modelle (Entwicklungsumgebung)

## 7.2 Ollama f√ºr Embedding- und Vision-Modelle (Entwicklungsumgebung)

### Schritt 1: Ollama installieren

```bash
# Ollama mit dem offiziellen Installationsskript installieren
# Ollama = benutzerfreundliche Plattform f√ºr lokale LLM-Ausf√ºhrung
curl -fsSL https://ollama.com/install.sh | sh

# Installation verifizieren
ollama --version
```
*Installiert **Ollama-Server** f√ºr einfache Modell-Verwaltung.*

### Schritt 2: Systemd-Service konfigurieren

```bash
# Sichere Systemd-Konfiguration erstellen
sudo nano /etc/systemd/system/ollama.service
```

**Service-Konfiguration einf√ºgen:**
```ini
[Unit]
Description=Ollama Service
Documentation=https://github.com/ollama/ollama
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve  # Startet Ollama-Server
User=ollama    # Eigener System-User f√ºr Sicherheit
Group=ollama
Restart=always  # Automatischer Neustart bei Absturz
RestartSec=3

# Umgebungsvariablen f√ºr Ollama-Konfiguration
Environment="PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
Environment="OLLAMA_HOST=127.0.0.1:11434"      # Nur localhost = sicherer
Environment="OLLAMA_MAX_LOADED_MODELS=2"       # Max 2 Modelle gleichzeitig im RAM
Environment="OLLAMA_NUM_PARALLEL=8"            # 8 parallele Anfragen pro Modell
Environment="OLLAMA_KEEP_ALIVE=5m"             # Modell 5 Min im RAM behalten
Environment="OLLAMA_MAX_QUEUE=128"             # Warteschlangengr√∂√üe
Environment="OLLAMA_CONTEXT_LENGTH=4096"       # Kontext-L√§nge f√ºr Gespr√§che

[Install]
WantedBy=multi-user.target
```

### Schritt 3: Service starten

```bash
# Systemd-Konfiguration neu laden und Service starten
sudo systemctl daemon-reload
sudo systemctl enable ollama    # Automatischer Start beim Boot
sudo systemctl start ollama     # Sofort starten

# Service-Status pr√ºfen
sudo systemctl status ollama
```
*Startet **Ollama-Server** im Hintergrund f√ºr Modell-Management.*

### Schritt 4: Embedding-Modell installieren

```bash
# Bestes multilinguales Embedding-Modell f√ºr deutsche/englische RAG-Systeme
# Quantisiert f√ºr effizienten Speicherverbrauch
ollama pull jeffh/intfloat-multilingual-e5-large:q8_0

# Modell-Download verifizieren
ollama list | grep multilingual
```
*Installiert **spezialisiertes RAG-Embedding-Modell** f√ºr Dokumentensuche.*

### Schritt 5: Hauptsprachmodell installieren (optional)

```bash
# Option 1: Gemma 3 12B (f√ºr Server mit 16GB+ VRAM)
ollama pull gemma3:12b-it-qat

# Option 2: Kleinere Alternative f√ºr weniger VRAM (< 12GB)
# Siehe https://ollama.com/search f√ºr aktuelle Modelle
# ollama pull gemma3:8b-it-qat    # F√ºr 8GB VRAM
# ollama pull llama3.2:3b-it      # F√ºr 4GB VRAM

# Verf√ºgbare Modelle anzeigen
ollama list
```
*Installiert **Entwicklungs-Sprachmodell** f√ºr Tests (parallel zu vLLM-Produktion).*

### Schritt 6: Installation verifizieren

```bash
# Alle installierten Modelle anzeigen
ollama list

# Embedding-Modell testen
echo "Hallo Welt" | ollama embeddings jeffh/intfloat-multilingual-e5-large:q8_0

# Service-Status final pr√ºfen
sudo systemctl status ollama

# Speicherverbrauch √ºberwachen
docker stats 2>/dev/null || echo "Ollama l√§uft nativ (nicht in Docker)"
```
*Ollama l√§uft **permanent** f√ºr Embedding/Vision-Aufgaben w√§hrend der Entwicklung.*

**Modell-√úbersicht:**
- **jeffh/intfloat-multilingual-e5-large:q8_0:** RAG-Embeddings (deutsch/englisch)
- **gemma3:12b-it-qat:** Entwicklungs-Sprachmodell (parallel zu vLLM)
- **Weitere Modelle:** https://ollama.com/search nach Bedarf

## 8. Suchfunktionen (Optional)

### Was ist Perplexica?

**Perplexica** ist eine **KI-gest√ºtzte Suchmaschine**, die:
- Internetsuchen mit **AI-Antworten** kombiniert
- **Quellen automatisch zusammenfasst** und bewertet
- **Multimodale Suche** (Text, Bilder, Videos) erm√∂glicht
- **Lokale LLMs** f√ºr Datenschutz verwendet

**Anwendung im AI-Tutor:** Studenten k√∂nnen komplexe Fragen stellen und erhalten strukturierte, quellenbasierte Antworten statt einzelner Suchergebnisse.

### Option A: Perplexica (Vollst√§ndige KI-Suchmaschine)

#### Schritt 1: Repository klonen

```bash
# Perplexica-Repository herunterladen
cd ~/docker
git clone https://github.com/ItzCrazyKns/Perplexica.git
cd Perplexica
```

#### Schritt 2: Konfiguration anpassen

```bash
# Konfigurationsdatei f√ºr lokale Modelle erstellen
cat > config.toml << 'EOF'
[API_KEYS]
OPENAI = ""
GROQ = ""
ANTHROPIC = ""
GEMINI = ""

[API_ENDPOINTS]
OLLAMA = "http://host.docker.internal:11434"   # Verbindung zu unserem Ollama
SEARXNG = "http://searxng:8080"               # Interne Suchmaschine

[GENERAL]
PORT = 3001
SIMILARITY_MEASURE = "cosine"
KEEP_ALIVE = "5m"
EOF
```

#### Schritt 3: Docker-Container starten

```bash
# Perplexica mit integrierter SearXNG starten
# SERVER_IP durch Ihre echte IP ersetzen!
SERVER_IP=$(hostname -I | awk '{print $1}')  # Dieser Befehl findet Ihre IP

cat > docker-compose.yml << EOF
version: '3.8'

services:
  searxng:
    image: docker.io/searxng/searxng:latest
    container_name: perplexica-searxng
    volumes:
      - ./searxng:/etc/searxng:rw
    ports:
      - "127.0.0.1:4000:8080"  # Nur localhost
    networks:
      - perplexica-network
    restart: unless-stopped

  perplexica-backend:
    build:
      context: .
      dockerfile: backend.dockerfile
    container_name: perplexica-backend
    environment:
      - SEARXNG_API_URL=http://searxng:8080
    depends_on:
      - searxng
    ports:
      - "127.0.0.1:3001:3001"  # Nur localhost
    volumes:
      - backend-dbstore:/home/perplexica/data
      - uploads:/home/perplexica/uploads
      - ./config.toml:/home/perplexica/config.toml:ro
    extra_hosts:
      - 'host.docker.internal:host-gateway'
    networks:
      - perplexica-network
    restart: unless-stopped

  perplexica-frontend:
    build:
      context: .
      dockerfile: app.dockerfile
      args:
        - NEXT_PUBLIC_API_URL=http://$SERVER_IP:3001/api
        - NEXT_PUBLIC_WS_URL=ws://$SERVER_IP:3001
    container_name: perplexica-frontend
    depends_on:
      - perplexica-backend
    ports:
      - "3000:3000"
    networks:
      - perplexica-network
    restart: unless-stopped

networks:
  perplexica-network:
    driver: bridge

volumes:
  backend-dbstore:
  uploads:
EOF

# Container starten
docker-compose up -d

# Status pr√ºfen
docker-compose ps
```
*Perplexica ist unter **http://YOUR_SERVER_IP:3000** erreichbar.*

### Option B: Nur SearXNG (Einfache Metasuchmaschine)

**F√ºr einfachere Setups ohne KI-Suchfunktionen:**

```bash
# Minimale SearXNG-Installation
mkdir -p ~/docker/searxng
cd ~/docker/searxng

# SearXNG-Konfiguration
cat > settings.yml << 'EOF'
use_default_settings: true

server:
  secret_key: "$(openssl rand -hex 32)"
  limiter: false
  image_proxy: true
  port: 8080
  bind_address: "0.0.0.0"

ui:
  static_use_hash: true

search:
  safe_search: 0
  autocomplete: ""
  default_lang: "de"
  formats:
    - html
    - json
EOF

# SearXNG-Container starten
docker run -d \
  --name searxng \
  --restart unless-stopped \
  -p 127.0.0.1:4000:8080 \
  -v $(pwd)/settings.yml:/etc/searxng/settings.yml:ro \
  searxng/searxng

# Status pr√ºfen
docker ps | grep searxng
```
*SearXNG ist unter **http://127.0.0.1:4000** erreichbar.*

### OpenWebUI-Integration

**Beide Optionen in OpenWebUI unter Websuche konfigurieren:**

```
# F√ºr Perplexica:
Suchmaschine: SearXNG
SearxNG-URL: http://host.docker.internal:4000/search?q=<query>

# F√ºr nur SearXNG:  
Suchmaschine: SearXNG
SearxNG-URL: http://host.docker.internal:4000/search?q=<query>

Suchergebnisse: 5
Gleichzeitige Anfragen: 8
```
*Aktiviert **Echtzeit-Internetsuche** f√ºr aktuelle Informationen.*

## 9. ComfyUI f√ºr Bildgenerierung (Optional)

### Was ist ComfyUI?

**ComfyUI** ist eine **node-basierte UI f√ºr Stable Diffusion**, die:
- **Workflow-basierte Bildgenerierung** erm√∂glicht
- **FLUX-Modelle** optimal unterst√ºtzt (inkl. FLUX.1-Kontext)
- **Modulare Pipelines** f√ºr komplexe Generierungen bietet
- **Bessere Performance** als WebUI Forge hat

### Installation (Manual Install)

```bash
# Python-Umgebung f√ºr ComfyUI erstellen
conda create --name comfyui python=3.10 -y
conda activate comfyui

# ComfyUI Repository klonen
cd ~/docker
git clone https://github.com/comfyanonymous/ComfyUI.git
cd ComfyUI

# Dependencies installieren
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt

# Installation testen
python main.py --help
```

### FLUX.1-Kontext Setup

**F√ºr optimale FLUX-Performance folgen Sie der detaillierten Anleitung:**

üîó **ComfyUI FLUX Tutorial:** https://comfyui-wiki.com/en/tutorial/advanced/image/flux/flux-1-kontext

**Wichtige FLUX-Modelle herunterladen:**
```bash
# Modell-Verzeichnis erstellen
mkdir -p models/checkpoints
mkdir -p models/vae
mkdir -p models/clip

# FLUX.1-dev Modell (falls verf√ºgbar und lizenziert)
# Hinweis: FLUX-Modelle haben spezielle Lizenzanforderungen
# wget -O models/checkpoints/flux1-dev.safetensors "FLUX_MODEL_URL"
```

### Systemd-Service f√ºr ComfyUI

```bash
# Service-Datei erstellen
SD_AUTH=$(cat ~/.ai-credentials/sd_auth)

sudo tee /etc/systemd/system/comfyui.service > /dev/null << EOF
[Unit]
Description=ComfyUI Service for AI Image Generation
After=network.target nvidia-persistenced.service

[Service]
Type=simple
User=aiservices
Group=aiservices
WorkingDirectory=/home/aiservices/ComfyUI
ExecStart=/bin/bash -c "source /root/miniconda3/etc/profile.d/conda.sh && conda activate comfyui && python main.py --listen --port 7870 --disable-auto-launch"
Restart=always
Environment="PATH=/root/miniconda3/envs/comfyui/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

# Sicherheitseinstellungen
NoNewPrivileges=true
ProtectSystem=strict
ReadWritePaths=/home/aiservices/ComfyUI

[Install]
WantedBy=multi-user.target
EOF

# ComfyUI-Installation zu aiservices-User kopieren
sudo cp -r ~/docker/ComfyUI /home/aiservices/
sudo chown -R aiservices:aiservices /home/aiservices/ComfyUI

# Service aktivieren
sudo systemctl daemon-reload
sudo systemctl enable comfyui
sudo systemctl start comfyui

# Status pr√ºfen
sudo systemctl status comfyui
```

### ComfyUI-Zugriff

```bash
# ComfyUI-Interface aufrufen:
# http://YOUR_SERVER_IP:7870

# Logs √ºberwachen
sudo journalctl -u comfyui -f

# Service-Status pr√ºfen
sudo systemctl status comfyui
```

### OpenWebUI-Integration

**ComfyUI als Bildgenerierungs-Backend konfigurieren:**

**Einstellungen > Bildgenerierung in OpenWebUI:**
```
Engine: ComfyUI
Basis-URL: http://host.docker.internal:7870/
API-Endpunkt: /api/prompt
```

### Erweiterte Konfiguration

**F√ºr FLUX.1-Kontext und andere erweiterte Features:**

üìñ **Vollst√§ndige Dokumentation:**
- **ComfyUI Manual Install:** https://github.com/comfyanonymous/ComfyUI#manual-install-windows-linux
- **FLUX.1-Kontext Tutorial:** https://comfyui-wiki.com/en/tutorial/advanced/image/flux/flux-1-kontext

**Hinweise:**
- ComfyUI ben√∂tigt **mindestens 12GB VRAM** f√ºr FLUX-Modelle
- **Workflow-Dateien** k√∂nnen in der Community geteilt werden
- **Custom Nodes** erweitern die Funktionalit√§t erheblich

*ComfyUI l√§uft **permanent** und bietet **professionelle Bildgenerierung** f√ºr Lehrmaterialien.*

## 11. Langfuse f√ºr AI-Telemetrie (Optional)

### Was ist Langfuse?

**Langfuse** ist eine **Open-Source-Plattform** f√ºr:
- **LLM-Observability:** √úberwachung von AI-Modell-Performance
- **Kosten-Tracking:** API-Aufrufe und Ressourcenverbrauch
- **Quality-Monitoring:** Antwortqualit√§t und Benutzer-Feedback  
- **Analytics:** Nutzungsstatistiken f√ºr Forschungszwecke

**Nutzen f√ºr den AI-Tutor:** Analysieren Sie, wie Studierende den Chatbot nutzen, welche Fragen gestellt werden und wie gut die Antworten sind.

### Langfuse mit OpenWebUI integrieren

#### Schritt 1: Langfuse-Container starten

```bash
# Langfuse mit PostgreSQL-Datenbank starten
docker run -d \
  --name langfuse \
  --restart unless-stopped \
  -p 127.0.0.1:3030:3000 \
  -e DATABASE_HOST=langfuse-db \
  -e DATABASE_USERNAME=langfuse \
  -e DATABASE_PASSWORD=$(openssl rand -base64 32) \
  -e DATABASE_NAME=langfuse \
  -e NEXTAUTH_SECRET=$(openssl rand -base64 32) \
  -e SALT=$(openssl rand -base64 32) \
  --network langfuse-net \
  langfuse/langfuse:latest

# PostgreSQL-Datenbank f√ºr Langfuse
docker run -d \
  --name langfuse-db \
  --restart unless-stopped \
  -e POSTGRES_DB=langfuse \
  -e POSTGRES_USER=langfuse \
  -e POSTGRES_PASSWORD=$(cat ~/.ai-credentials/main_api_key | head -c 32) \
  -v langfuse-db:/var/lib/postgresql/data \
  --network langfuse-net \
  postgres:15

# Docker-Netzwerk erstellen
docker network create langfuse-net 2>/dev/null || true

# Services starten
docker start langfuse-db langfuse

# Status pr√ºfen
docker ps | grep langfuse
```

#### Schritt 2: Langfuse konfigurieren

```bash
# Langfuse-Webinterface aufrufen: http://127.0.0.1:3030
# Ersten Admin-Account erstellen

# API-Keys f√ºr OpenWebUI-Integration generieren
echo "1. Gehen Sie zu http://127.0.0.1:3030"
echo "2. Erstellen Sie einen Account"
echo "3. Navigieren Sie zu Settings > API Keys"
echo "4. Erstellen Sie neue API-Keys f√ºr OpenWebUI"
echo "5. Notieren Sie: Public Key und Secret Key"
```

#### Schritt 3: OpenWebUI mit Langfuse verbinden

**Folgen Sie der offiziellen Anleitung:** https://langfuse.com/integrations/no-code/openwebui

**In OpenWebUI konfigurieren:**

1. **Admin-Panel > Einstellungen > Externe Verbindungen**
2. **Langfuse-Integration aktivieren:**
```
Langfuse Host: http://127.0.0.1:3030
Public Key: [Aus Langfuse kopieren]
Secret Key: [Aus Langfuse kopieren]
```

3. **Telemetrie-Einstellungen:**
```
‚òë Conversation Tracking
‚òë Model Performance Monitoring  
‚òë User Feedback Collection
‚òë Cost Tracking (falls API-basiert)
```

#### Schritt 4: Monitoring-Dashboard einrichten

```bash
# Langfuse-Analytics aufrufen
echo "Analytics verf√ºgbar unter: http://127.0.0.1:3030/analytics"

# Beispiel-Metriken die erfasst werden:
echo "üìä Verf√ºgbare Metriken:"
echo "- Anzahl Gespr√§che pro Tag/Woche"
echo "- Durchschnittliche Antwortzeit"
echo "- H√§ufigste Fragen/Themen"
echo "- Benutzer-Feedback-Scores"
echo "- Modell-Performance-Vergleiche"
echo "- Ressourcenverbrauch (GPU/RAM)"
```

### Datenschutz-Konfiguration

```bash
# Datenschutz-konforme Einstellungen
cat > ~/langfuse-privacy.conf << 'EOF'
# Langfuse Datenschutz-Konfiguration f√ºr Hochschule

# Anonymisierung aktivieren
LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES=true
LANGFUSE_ANONYMIZE_USER_DATA=true

# Datenaufbewahrung (DSGVO-konform)
LANGFUSE_DATA_RETENTION_DAYS=365  # 1 Jahr f√ºr Forschungszwecke

# IP-Adressen nicht loggen
LANGFUSE_LOG_IP_ADDRESSES=false

# Sensible Daten filtern
LANGFUSE_FILTER_PERSONAL_DATA=true
EOF

echo "‚ö†Ô∏è WICHTIG: Datenschutz-Richtlinien der Hochschule beachten!"
echo "üìã DSGVO-Compliance sicherstellen vor Produktiveinsatz!"
```

### Analytics f√ºr Forschung

**M√∂gliche Forschungsfragen mit Langfuse-Daten:**
- Welche Fachbereiche nutzen den AI-Tutor am meisten?
- Zu welchen Uhrzeiten ist die Nutzung am h√∂chsten?
- Welche Fragetypen f√ºhren zu den besten Bewertungen?
- Wie entwickelt sich die Antwortqualit√§t √ºber Zeit?

```bash
# Export f√ºr Forschungsanalyse
echo "Daten-Export f√ºr Forschung:"
echo "1. Langfuse > Analytics > Export"
echo "2. CSV/JSON-Format f√ºr statistische Analyse"
echo "3. Anonymisierte Daten f√ºr Publikationen"
```

*Langfuse erm√∂glicht **wissenschaftlich fundierte Evaluierung** des AI-Tutors f√ºr das Forschungsprojekt.*

### Modellverbindungen einrichten

**Nach dem ersten Start von OpenWebUI im Browser (http://YOUR_SERVER_IP):**

1. **Admin-Account erstellen** beim ersten Besuch der Website
2. **SIGNUP deaktivieren** unter Admin-Einstellungen (verhindert weitere Registrierungen)
3. **Administrationsbereich > Einstellungen > Verbindungen** aufrufen

#### vLLM-Verbindung (Gemma 3):
```
URL: http://host.docker.internal:8000/v1
API-Key: [Inhalt aus ~/.ai-credentials/main_api_key kopieren]
Modell: gemma-3-12b
```
*Diese Verbindung erm√∂glicht Zugriff auf das **Produktions-Sprachmodell** f√ºr Lehrzwecke.*

#### Ollama-Verbindung:
```
URL: http://host.docker.internal:11434
```
*Aktiviert **Vision-Modelle** und **Embedding-Funktionen** f√ºr RAG-Systeme.*

### Embedding-Modell konfigurieren

**Dokumente > Einstellungen:** *(Erm√∂glicht Upload und Durchsuchung von PDF-Dokumenten)*
```
Embedding-Engine: Ollama                    # Verwendet Ollama f√ºr Texteinbettungen
URL: http://host.docker.internal:11434      # Interne Container-Verbindung
Embedding-Modell: granite-embedding:278m    # IBM's spezialisiertes Modell
Stapelgr√∂√üe: 16                             # Verarbeitet 16 Textbl√∂cke gleichzeitig
Top K: 5                                    # Zeigt 5 relevanteste Suchergebnisse
Blockgr√∂√üe: 800                             # 800 Zeichen pro Textblock
Block√ºberlappung: 100                       # 100 Zeichen √úberlappung zwischen Bl√∂cken
```
*Konfiguration f√ºr **RAG-Funktionalit√§t** - erm√∂glicht Wissensabfrage aus eigenen Dokumenten.*

### Websuche aktivieren

**Einstellungen > Websuche:** *(Erweitert Chatbot um aktuelle Internetinformationen)*
```
Suchmaschine: SearXNG                                              # Privacy-fokussierte Metasuchmaschine
SearxNG-URL: http://host.docker.internal:4000/search?q=<query>     # Interne Suchservice-Verbindung
Suchergebnisse: 5                                                  # Anzahl der Suchergebnisse pro Anfrage
Gleichzeitige Anfragen: 8                                          # Parallel-Suchen f√ºr bessere Performance
```
*Aktiviert **Echtzeit-Internetsuche** f√ºr aktuelle Informationen und Faktencheck.*

### Bildgenerierung konfigurieren

**Einstellungen > Bildgenerierung:** *(F√ºgt AI-Bildgenerierung zum Chatbot hinzu)*
```
Engine: Automatic1111                                    # Kompatibel mit Stable Diffusion Forge
Basis-URL: http://host.docker.internal:7870/            # Interne Verbindung zu SD Forge
API-Auth: [Inhalt aus ~/.ai-credentials/sd_auth]        # Benutzername:Passwort f√ºr Authentifizierung
```
*Erm√∂glicht **Bildgenerierung** direkt im Chat f√ºr visuelle Lehrmaterialien.*

### Code-Interpreter aktivieren

**Einstellungen > Code Execution:** *(Aktiviert Programmcode-Ausf√ºhrung im Chat)*
```
Engine: Jupyter                                         # Jupyter Notebook als Code-Umgebung
URL: http://host.docker.internal:8888                   # Jupyter-Server-Verbindung
Token: [Inhalt aus ~/.ai-credentials/jupyter_token]     # Sicherheits-Token f√ºr Jupyter-Zugriff
```
*Erlaubt **Code-Ausf√ºhrung** f√ºr Datenanalyse, Berechnungen und Programmierung im Chat.*

## 12. Wartung und Monitoring

### System√ºberwachung

```bash
# System-Monitoring-Script erstellen f√ºr regelm√§√üige √úberwachung
cat > ~/monitor-ai-stack.sh << 'EOF'
#!/bin/bash
echo "=== AI Stack Status Check ==="
echo "Date: $(date)"
echo

# Zeigt verf√ºgbaren RAM und Swap-Speicher
echo "=== System Resources ==="
free -h
echo

# Zeigt verf√ºgbaren Festplattenspeicher (Root-Partition)
df -h /
echo

# GPU-Status: Auslastung und Speicherverbrauch
echo "=== GPU Status ==="
nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits
echo

# Status aller AI-Services pr√ºfen
echo "=== Service Status ==="
echo "vLLM Gemma: $(sudo systemctl is-active vllm-gemma)"
echo "Ollama: $(sudo systemctl is-active ollama)"
echo "Stable Diffusion: $(sudo systemctl is-active sd-forge 2>/dev/null || echo 'not-installed')"

echo
# √úbersicht aller laufenden Docker-Container mit Image-Versionen
echo "=== Docker Containers ==="
docker ps --format "table {{.Names}}\t{{.Image}}\t{{.Status}}\t{{.Ports}}"

echo
# Zeigt welche Services auf welchen Ports lauschen
echo "=== Port Status ==="
sudo netstat -tlnp | grep -E ':(80|8000|11434|3000|7870|8888) '

echo
# OpenWebUI Update-History anzeigen (letzte 5 Eintr√§ge)
echo "=== Recent OpenWebUI Updates ==="
if [ -f /var/log/openwebui-updates.log ]; then
    tail -5 /var/log/openwebui-updates.log
else
    echo "No update log found yet (updates run Fridays at 16:00)"
fi
EOF

# Script ausf√ºhrbar machen
chmod +x ~/monitor-ai-stack.sh

# Script ausf√ºhren um aktuellen Status zu sehen
~/monitor-ai-stack.sh

# Optional: Monitoring-Script als t√§glichen Cron-Job einrichten (8:00 morgens)
# (crontab -l 2>/dev/null; echo "0 8 * * * ~/monitor-ai-stack.sh >> /var/log/ai-stack-monitor.log 2>&1") | crontab -
```
*Dieses Script wird **t√§glich** oder bei Problemen ausgef√ºhrt f√ºr System√ºberwachung.*

### Watchtower Update-Management

```bash
# Watchtower-Status und verf√ºgbare Updates pr√ºfen (ohne Update durchzuf√ºhren)
docker run --rm \
  --volume /var/run/docker.sock:/var/run/docker.sock \
  containrrr/watchtower \
  --run-once --dry-run open-webui

# Alle Container-Images auf Updates pr√ºfen
docker images --format "table {{.Repository}}\t{{.Tag}}\t{{.CreatedAt}}\t{{.Size}}" | head -10

# Update-Log-Datei √ºberwachen (zeigt letzten Updates)
tail -20 /var/log/openwebui-updates.log

# Aktuell installierte OpenWebUI-Version anzeigen
docker inspect open-webui --format='{{.Config.Labels.version}}{{.Config.Labels.build_date}}'
```
*Diese Befehle helfen bei der **Update-√úberwachung** und zeigen verf√ºgbare Aktualisierungen.*

### Backup-Strategie

```bash
# Backup-Script erstellen
cat > ~/backup-ai-config.sh << 'EOF'
#!/bin/bash
BACKUP_DIR="/backup/ai-stack-$(date +%Y%m%d-%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "Backing up AI stack configuration to $BACKUP_DIR"

# Systemd-Services
sudo cp /etc/systemd/system/vllm-main.service "$BACKUP_DIR/"
sudo cp /etc/systemd/system/ollama.service "$BACKUP_DIR/"
sudo cp /etc/systemd/system/sd-forge.service "$BACKUP_DIR/"

# Credentials (verschl√ºsselt)
tar -czf "$BACKUP_DIR/credentials.tar.gz" -C ~/.ai-credentials .

# Docker-Konfigurationen
cp -r ~/docker "$BACKUP_DIR/"

# OpenWebUI-Daten
docker run --rm -v open-webui-data:/data -v "$BACKUP_DIR":/backup ubuntu \
    tar czf /backup/openwebui-data.tar.gz -C /data .

echo "Backup completed: $BACKUP_DIR"
EOF

chmod +x ~/backup-ai-config.sh
```

### Updates und Wartung

```bash
# Update-Script erstellen
cat > ~/update-ai-stack.sh << 'EOF'
#!/bin/bash
echo "Updating AI Stack..."

# System-Updates
sudo apt update && sudo apt upgrade -y

# Docker-Images aktualisieren
docker-compose -f ~/docker/openwebui/docker-compose.yml pull
docker-compose -f ~/docker/vllm-compose.yml pull

# Container neu starten
docker-compose -f ~/docker/openwebui/docker-compose.yml up -d
docker-compose -f ~/docker/vllm-compose.yml up -d

# Ollama-Modelle aktualisieren
ollama list | tail -n +2 | awk '{print $1}' | xargs -I {} ollama pull {}

echo "Update completed"
EOF

chmod +x ~/update-ai-stack.sh
```

## 13. Troubleshooting

### H√§ufige Probleme

#### Service startet nicht:
```bash
# Detaillierte Logs der letzten 50 Zeilen anzeigen
# Zeigt Fehlermeldungen beim Service-Start
sudo journalctl -u SERVICE_NAME -n 50 --no-pager

# Beispiel f√ºr vLLM-Service:
sudo journalctl -u vllm-gemma -n 50 --no-pager

# Dateiberechtigungen f√ºr Credentials pr√ºfen
# User muss API-Key-Datei lesen k√∂nnen
ls -la ~/.ai-credentials/
sudo ls -la /home/aiservices/.ai-credentials/

# Service manuell starten um Fehler zu sehen
sudo systemctl start vllm-gemma
sudo systemctl status vllm-gemma  # Zeigt aktuellen Status
```
*Diese Befehle helfen bei der **Fehlerdiagnose** wenn Services nicht starten.*

#### GPU-Speicherprobleme (Gemma 3 ben√∂tigt ~16GB VRAM):
```bash
# GPU-Speicher und -Auslastung in Echtzeit √ºberwachen
# Zeigt welche Prozesse GPU verwenden und wie viel VRAM belegt ist
watch -n 1 nvidia-smi

# Docker-Container-Ressourcenverbrauch anzeigen
# Zeigt CPU, RAM und GPU-Nutzung aller Container
docker stats

# Falls zu wenig VRAM: vLLM-Parameter in Systemd-Service anpassen
# Folgende Zeilen in /etc/systemd/system/vllm-gemma.service √§ndern:
# --max-model-len 2048 (statt 4096) = k√ºrzerer Kontext
# --max-num-seqs 8 (statt 16) = weniger parallele Anfragen
# --gpu-memory-utilization 0.8 (statt 0.9) = weniger GPU-RAM nutzen

# Nach √Ñnderungen Service neu laden:
sudo systemctl daemon-reload
sudo systemctl restart vllm-gemma
```
*Wird **bei Performance-Problemen** verwendet. GPU-Monitoring ist essentiell.*

#### Netzwerk-Verbindungsprobleme:
```bash
# Pr√ºfen welche Prozesse auf welchen Ports lauschen
# Zeigt ob vLLM auf Port 8000 erreichbar ist
sudo netstat -tlnp | grep :8000

# Firewall-Status pr√ºfen - zeigt alle ge√∂ffneten Ports
sudo ufw status

# Netzwerk-Konnektivit√§t zwischen Containern testen
# Testet ob Container andere Services erreichen k√∂nnen
docker exec -it CONTAINER_NAME curl http://host.docker.internal:8000/health

# API-Funktionalit√§t direkt testen
# Sendet Test-Anfrage an Gemma 3 Modell
curl -X POST "http://127.0.0.1:8000/v1/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $(cat ~/.ai-credentials/main_api_key)" \
  -d '{"model": "gemma-3-12b", "prompt": "Test", "max_tokens": 10}'

# Erfolgreiche Antwort sollte JSON mit "choices" enthalten
```
*Diese Tests pr√ºfen **API-Konnektivit√§t** und **Service-Erreichbarkeit**.*

### Logs und Debugging

```bash
# Zentrales Logging einrichten
sudo tee /etc/rsyslog.d/50-ai-stack.conf > /dev/null << 'EOF'
# AI Stack Logs
:programname, startswith, "vllm" /var/log/ai-stack/vllm.log
:programname, startswith, "ollama" /var/log/ai-stack/ollama.log
& stop
EOF

sudo mkdir -p /var/log/ai-stack
sudo systemctl restart rsyslog

# Live-Monitoring aller Services
sudo journalctl -f -u vllm-gemma -u ollama -u sd-forge
```

### Gemma 3 Performance-Tuning

```bash
# GPU-spezifische Optimierungen in systemd-Service hinzuf√ºgen
# Diese Umgebungsvariablen verbessern GPU-Performance erheblich

# F√ºr RTX 4090 (24GB VRAM): Maximale Performance-Konfiguration
# In /etc/systemd/system/vllm-gemma.service anpassen:
--max-model-len 8192              # L√§ngere Kontexte m√∂glich
--max-num-seqs 32                 # Viele parallele Anfragen
--max-num-batched-tokens 16384    # Gro√üe Batch-Gr√∂√üe
--gpu-memory-utilization 0.95     # Fast gesamten GPU-RAM nutzen

# F√ºr RTX 3080/3090 (12-24GB VRAM): Ausgewogene Konfiguration
--max-model-len 4096              # Standard-Kontext
--max-num-seqs 16                 # Moderate parallele Anfragen
--max-num-batched-tokens 8192     # Standard-Batch-Gr√∂√üe
--gpu-memory-utilization 0.9      # 90% GPU-RAM

# F√ºr RTX 3070 (8GB VRAM): Nicht empfohlen f√ºr Gemma 3 12B!
# Verwenden Sie stattdessen Gemma 3 8B oder kleinere Modelle:
# RedHatAI/gemma-3-8b-it-quantized.w8a8

# Erweiterte Performance-Umgebungsvariablen hinzuf√ºgen:
Environment="CUDA_LAUNCH_BLOCKING=0"        # Reduziert GPU-Overhead
Environment="CUDA_CACHE_DISABLE=0"          # Aktiviert CUDA-Kernel-Cache
Environment="VLLM_ATTENTION_BACKEND=FLASHINFER"  # F√ºr RTX 30/40-Serie

# Nach √Ñnderungen Service neu starten:
sudo systemctl daemon-reload
sudo systemctl restart vllm-gemma

# Performance √ºberwachen:
nvidia-smi dmon -s pucvmet -d 2  # GPU-Metriken alle 2 Sekunden
```
*Diese Optimierungen werden **je nach GPU-Hardware** angepasst und erh√∂hen Durchsatz.*

## 14. Sicherheits-Checkliste

### Vor Produktivbetrieb pr√ºfen:

- [ ] Alle `YOUR_*` Platzhalter durch echte Werte ersetzt
- [ ] Starke, eindeutige Passw√∂rter und API-Keys generiert
- [ ] Firewall korrekt konfiguriert (nur notwendige Ports offen)
- [ ] Services laufen mit dedizierten Benutzern (nicht root)
- [ ] SSL/TLS f√ºr externe Zugriffe konfiguriert
- [ ] Backup-Strategie implementiert
- [ ] Monitoring und Logging aktiviert  
- [ ] Automatische Sicherheitsupdates aktiviert
- [ ] Fail2Ban f√ºr SSH konfiguriert
- [ ] Credentials niemals in Code/Logs committen

### SSL/TLS mit Let's Encrypt (Optional)

```bash
# Certbot installieren
sudo apt install -y certbot

# SSL-Zertifikat f√ºr OpenWebUI
sudo certbot certonly --standalone -d YOUR_DOMAIN.com

# Nginx als Reverse Proxy (Optional)
sudo apt install -y nginx
# Nginx-Konfiguration f√ºr SSL-Termination erstellen
```

---

**Erstellt:** $(date)  
**Version:** 2.1 (Security-Enhanced, Gemma 3, Updated Drivers)  
**Wartung:** Regelm√§√üige Updates erforderlich

‚ö†Ô∏è **WICHTIG:** Diese Konfiguration ist f√ºr Produktionsumgebungen optimiert. Alle Credentials m√ºssen vor der Nutzung angepasst werden!

### Modell-Spezifikationen:
- **vLLM (Produktiv):** Gemma 3 12B INT8 (~16GB VRAM)
- **Ollama (Development):** jeffh/intfloat-multilingual-e5-large:q8_0 (RAG), gemma3:12b-it-qat
- **GPU-Treiber:** NVIDIA 570.x/575.x
- **CUDA:** 12.9 Update 1
- **Bildgenerierung:** ComfyUI mit FLUX.1-Kontext-Support
- **Telemetrie:** Langfuse f√ºr Forschungsanalyse